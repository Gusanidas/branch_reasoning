
Take a look at the file "references/trainer.py". It is a reference of the file I want you to implement, but there are important differences.

We are going to write the new file in "branch_reasoning/training/trainer.py".

The new trainer function is going to have as arguments:
 - model: The model to train.
 - tokenizer: The tokenizer of the model.
 - List[PromptCompletion]: A list of PromptCompletion objects. (Defined in "branch_reasoning/generation/completions.py"), this is intstead of "dataloader" as in the reference.
 - optimizer: The optimizer to use for training.
 - scheduler: The scheduler to use for training.
 - gradient_accumulation_steps: The number of steps to accumulate gradients, but in the new file it is going to be the number of prompts per batch. Each prompt has several completions.
 - device: The device to run the training on.
 - iteration: The current iteration number for logging.
 - use_wandb: Whether to use wandb for logging.
 - training_args: A dict containing:
    - num_epochs: The number of epochs to train the model.
    - epsilon_low: The lower clipping threshold for importance weight.
    - epsilon_high: The upper clipping threshold for importance weight.
    - max_grad_norm: The maximum gradient norm for clipping.
    - temperature: The temperature for the softmax.
    - beta

Ignore arguments like:
 - loss_multiplier
 - top_k
 - log_prob_bias
 
 Other thing to have in mind:

 - Divide the loss by gradient_accumulation_steps * completions_per_prompt.
 - The log_probs and ref_log_probs are stored in the Branch objects.
 - Log similar things as in the reference file.
 - The "Advantages" are the "scores".
 
 

 
 